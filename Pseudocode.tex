\documentclass[a4paper]{article}

\usepackage[english]{babel}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{cite}

\usepackage{cases}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}


\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
 }
 \usepackage{physics}

\begin{document}

\begin{enumerate}
\item Classification
	\begin{enumerate}
		\item  K-Nearest Neighbor 
	\end{enumerate}

\item Ensemble Algorithms
	\begin{enumerate}
		\item Adaboost 
	\end{enumerate} 
\end{enumerate}


  \begin{algorithm}
 
   \caption{k-Nearest Neighbor  ~ \cite{knearest91} }
    \begin{algorithmic}[1]
    \INPUT{X: training data, Y:Class labels of X, $x$: unknown sample}
     \OUTPUT{Class with the highest number of occurrence}
      \Function{Classify}{$X,Y,x$}\
       \For{$i = 1$ to ${m}$}
            \State Compute distance $d{(X_i,x)}$
        \EndFor
        
         \State Compute set $I$ containing indices for the $k$ smallest distances $d{(X_i,x)}$
     
       \State Return majority label $\{Y_i \ \text{where}\ i \in I \}$



 \EndFunction

\end{algorithmic}
\end{algorithm}



  \begin{algorithm}
   \caption{Adaboost  ~ \cite{adaboostfirst}}
    \begin{algorithmic}[1]
    \INPUT 
    \Statex Training data $\{(x_i,y_i)_{i=1}^N \ \text{where}\ x_i \in  \mathbb{R}^k \ \text{and} \  y_i \in \{-1,1\} \}$
    \Statex Large number of classifiers denoted by $f_m(x) \in \{-1,1\} $
    \Statex 0-1 loss function $I$ defined as 
 \begin{numcases}{ I(f_m(x,y))=}
  0, & if $ f_m(x_i) = y_i $\\
  1, &  if $ f_m(x_i) \neq y_i $
\end{numcases}
     \OUTPUT{The final classifier}
     
       \For{$i = 1$ to ${N}$}
       \For{$i = 1$ to ${M}$}
            \State Fit weak classifier m to minimize the objective function:
            \State $\epsilon_m =  \frac{\sum_{i=1}^N w_{i}^m I(f_m(x_i)) \neq y_i}{x^2+2x+1} $
            \State where  $I(f_m(x_i) \neq y_i) =1 \  \text{if} \ f_m(x_i) \neq y_i  $ and 0 otherwise
            \State $\alpha_m = \ln \frac{1- \epsilon_m}{\epsilon_m}$
            
        \EndFor
         \For{ all $i$ }
              \State $w_{i}^ {m+1} = w_{i} ^{(m)} e^{\alpha_{mI(f_m(x_i) \neq y_i)}} $ 
         \EndFor 
         
        
        \EndFor
        
        


\end{algorithmic}
\end{algorithm}




  \begin{algorithm}
   \caption{Adaboost ~\cite{adaboostsecond}}
    \begin{algorithmic}[1]
    \INPUT 
    \Statex Training data $\{(x_i,y_i)_{i=1}^N \ \text{where}\ x_i \in  \mathbb{R}^k \ \text{and} \  y_i \in \{-1,1\} \}$
   
     \OUTPUT{The final classifier}
    \State Given Training data $\{(x_i,y_i) \ \text{where}\  \  y_i \in \{-1,1\} \}$
    \State initialize $D_1$ = uniform distribution on training examples
       \For{$t = 1$ to ${T}$}
   
            \State Train weak classifier $h_t \  \text{on}\   \   D_t $
            \State choose $\alpha_t > 0 $
            \State compute new distribution $D_{t+1}$:
             \For{ all $i$ }
              \State multiply $D_t(x)$ by \begin{numcases}{}
  e^{-\alpha_t}, &  ($<1$) $ \text{if}\  \  y_i = h_t(x_i) $\\
   e^{\alpha_t}, & ($>1$) $ \text{if}\  \  y_i \neq h_t(x_i) $
\end{numcases}
\State renormalize
         \EndFor 
         
  \State output final classifier $H_final(x) = sign (\sum\alpha_t h_t(x))$
            
        
        
         
        
        \EndFor
        
        


\end{algorithmic}
\end{algorithm}



  \begin{algorithm}
   \caption{Random forest ~\cite{randomforest1}}
    \begin{algorithmic}[1]
    \INPUT{S: training set, F:Features and number of trees in forest $B$}
     \OUTPUT{Constructed tree}
      \Function{RANDOMFOREST}{$S,F$}\
      \State $H \leftarrow  \emptyset $
       \For{$i \in 1,....B$ }
            \State $S^{(i)}\leftarrow \text{A bootstrap sample from} \  S $
            \State $h_i \leftarrow RANDOMIZEDTREELEARN(S^{i},F)$
            \State $H \leftarrow H \bigcup \{h_i\}$
        \EndFor
        \State return $H$
         



 \EndFunction

  \Function{RANDOMIZEDTREELEARN}{$S,F$}\ 
  \State At each node:
  \State $f \leftarrow \text{a very small subset of} \ F $
  \State Split on best feature in $f$
  \State return The learned tree
   \EndFunction

\end{algorithmic}
\end{algorithm}





  \begin{algorithm}
   \caption{Iterative Dichotomiser 3}
    \begin{algorithmic}[1]
    \INPUT{$D:\text{Training Data}, \ X: \text{Set of Input  Attributes} $}
     \OUTPUT{A decision tree}
      \Function{ID3}{$D,X$}\
      \State Let $T$ be a new tree 
     \If {all instances in $D$ have the same class $c$}
      \State Label ($T$) = $c$; Return $T$
      \EndIf
      \If {$X= \emptyset\ \text{or no attribute has positive information gain} $}
      \State Label ($T$) = most common class in $D$; Return $T$
      \EndIf
      \State $X \leftarrow \ \text{attribute with highest information gain} $
      \State Label($T$) = $X$
       \For{$\text{each value} \ x \  \text{of} \ X$ }
            \State $D_x \leftarrow \ \text{instances in} \ D \ \text{with} \ X = x $
           \If {$D_x \ \text{is empty}$}
           \State Let $T_x \ \text{be a new tree}$
           \State Label($T_x$) = most common class in $D$
           \Else {\State $T_x$ = ID3($D_x,X-\{x\}$)}
           \EndIf
            \State Add a branch from $T$ to $T_x$ labeled by $x$
        \EndFor
        \State return $T$
         



 \EndFunction

 
\end{algorithmic}
\end{algorithm}


  \begin{algorithm}
   \caption{Perceptron ~\cite{perceptron1}}
    \begin{algorithmic}[1]
    \INPUT{$ProblemSize,InputPatterns,iterations_max,learn_rate$}
     \OUTPUT{$Weights$}
    
       \For{$i = 1$ to ${iterations_{max}}$}
            \State $Pattern_i \leftarrow SelectInputPattern(InputPatterns)$
            \State $Activation_i \leftarrow ActivateNetwork(Pattern_i,Weights)$
            \State $Output_i \leftarrow TransferActivation(Activation_i)$
            \State $UpdateWeights(Pattern_i,Output_i,learn_{rate})$
        \EndFor
        
       
       \State Return $Weights$



 

\end{algorithmic}
\end{algorithm}


  \begin{algorithm}
   \caption{Back-propagation  ~\cite{backpropagation12}}
    \begin{algorithmic}[1]
    \INPUT{$ProblemSize,InputPatterns,iterations_max,learn_rate$}
     \OUTPUT{$Network$}
     \State $Network \leftarrow ConstructNetworkLayers()$
     \State $Network_weights \leftarrow InitializeWeights(Network,ProblemSize)$
    
       \For{$i = 1$ to ${iterations_{max}}$}
            \State $Pattern_i \leftarrow SelectInputPattern(InputPatterns)$
            \State $Output_i \leftarrow ForwardPropagate(Pattern_i,Network)$
            \State $BackwardPropagateError(Pattern_i,Output_i,Network)$
            \State $UpdateWeights(Pattern_i,Output_i,Network,learn_{rate})$
        \EndFor
        
       
       \State Return $Network$



 

\end{algorithmic}
\end{algorithm}



  \begin{algorithm}
   \caption{Learning Vector Quantization ~\cite{learningvector3}}
    \begin{algorithmic}[1]
    \INPUT{$ProblemSize,InputPatterns,iterations_{max},CodebookVectors_{num},learn_{rate}$}
     \OUTPUT{$CodebookVectors$}
     \State $CodebookVectors \leftarrow InitializeCodebookVectors(CodebookVectors_{num},ProblemSize) $
     
    
       \For{$i = 1$ to ${iterations_{max}}$}
            \State $Pattern_i \leftarrow SelectInputPattern(InputPatterns)$
            \State $Bmu_i \leftarrow SelectBestMatchingUnit(Pattern_i,CodebookVectors)$
            \For{$Bmu_i ^{attribute} \in Bmu_i $}
            \If{$Bmu_i^{class} \equiv Pattern_i^{class} $}
            \State $Bmu_i^{attribute} \leftarrow Bmu_i^{attribute} + learn_{rate} \times (Pattern_i ^{attribute} - Bmu_i ^{attribute})  $
            \Else { \State $Bmu_i^{attribute} \leftarrow Bmu_i^{attribute} - learn_{rate} \times (Pattern_i ^{attribute} - Bmu_i ^{attribute})  $}
            \EndIf 
            \EndFor  
        \EndFor
        
       
       \State Return $CodebookVectors$



 

\end{algorithmic}
\end{algorithm}



  \begin{algorithm}
   \caption{Self Organizing Map ~\cite{som3} }
    \begin{algorithmic}[1]
    \INPUT{$InputPatterns,iterations_{max},learn_{rate},Grid_width,Grid_height$}
     \OUTPUT{$CodebookVectors$}
     \State $CodebookVectors \leftarrow InitializeCodebookVectors(Grid_{width},Grid_{height},InputPatterns) $
     
    
       \For{$i = 1$ to ${iterations_{max}}$}
            \State $Learn_{rate}^i \leftarrow CalculateLearningRate(i,learn_{rate}^{init})$
            \State $neighborhood_{size}^i \leftarrow CalculateNeighborhoodSize(i,neighborhood_{init}^{size})$
            \State $Pattern_i \leftarrow SelectInputPattern(InputPatterns)$
             \State $Bmu_i \leftarrow SelectBestMatchingUnit(Pattern_i,CodebookVectors)$
             \State $Neighborhood \leftarrow Bmu_i$
              \State $Neighborhood \leftarrow  SelectNeighbors(Bmu_i,CodebookVectors,neighborhood_{size}^i)$
            \For{$Vector_i  \in Neighborhood $}
            \For{$Vector_i^{attribute}  \in Vector_i $}
            \State $Vector_i^{attribute} \leftarrow Vector_i^{attribute} + learn_{rate} \times (Pattern_i ^{attribute} - Vector_i ^{attribute})  $
            \EndFor
          
            
            \EndFor
           
        \EndFor
        
       
       \State Return $CodebookVectors$



 

\end{algorithmic}
\end{algorithm}


  \begin{algorithm}
   \caption{Hierarchial Agglomerative Algorithm ~\cite{haa1}}
    \begin{algorithmic}[1]
    \INPUT{\Statex $\langle{V,E,w}\rangle.\text{Weighted graph}$
    \Statex $d_c. \text{Distance measure for two clusters}$}
     \OUTPUT{$\langle{V_T,E_T}\rangle.\text{Cluster hierarchy or dendogram}$}
     \State $C = \{\{v\mid v \in V\}\} $ \Comment{Initial Clustering}
     \State $V_t = \{v_C\mid C \in C\},E_T = \emptyset$ \Comment{Initial Dendogram}
     \While{$\abs{C} > 1$}
     \State $update\_distance\_matrix(C,G,d_c)$
     \State $\{C,C'\} =  \underset{\{C_i,C_j\} \in C : C_i \neq C_j}{argmin} d_c (C_i,C_j)$
     \State $C = (C \backslash \{C,C'\}) \cup \{C \cup C'\}$ \Comment{Merging}
     \State $V_T = V_T \cup \{v_{C,C'}\},E_T = E_T \cup \{\{v_{C,C'} ,v_{C}\},\{v_{C,C'} ,v_{C}\}\}$ \Comment{Dendogram}
     \EndWhile
    
      
        
       
       \State Return $T$



 

\end{algorithmic}
\end{algorithm}



  \begin{algorithm}
   \caption{Hierarchial Divisive Algorithm ~\cite{hda1}}
    \begin{algorithmic}[1]
    \INPUT{\Statex $\langle{V,E,w}\rangle.\text{Weighted graph}$
    \Statex $d_c. \text{Distance measure for two clusters}$}
     \OUTPUT{$\langle{V_T,E_T}\rangle.\text{Cluster hierarchy or dendogram}$}
     \State $C = \{ V\} $ \Comment{Initial Clustering}
     \State $V_t = \{v_C\mid C \in C\},E_T = \emptyset$ \Comment{Initial Dendogram}
     \While{$\exists{C_x}:(C_x \in C \wedge \abs{C} > 1)$}
     \State $update\_distance\_matrix(C,G,d_c)$
     \State $\{C,C'\} =  \underset{\{C_i,C_j\}  : C_i \cup C_j = C_x   \wedge \  C_i \cap C_j = \emptyset}{argmax} d_c (C_i,C_j)$
     \State $C = (C \backslash \{C,C'\}) \cup \{C \cup C'\}$ \Comment{Merging}
     \State $V_T = V_T \cup \{v_{C,C'}\},E_T = E_T \cup \{\{v_{C,C'} ,v_{C}\},\{v_{C,C'} ,v_{C}\}\}$ \Comment{Dendogram}
     \EndWhile
    
      
        
       
       \State Return $T$



 

\end{algorithmic}
\end{algorithm}


 \begin{algorithm}
 
   \caption{C4.5  ~\cite{c4.5} }
    \begin{algorithmic}[1]
    \INPUT{
    \Statex $T : \text{Training dataset} $
    \Statex $S : \text{Attributes} $
    }
     \OUTPUT{decision tree $Tree$}
     \Function{C4.5}{$T$}\
      \If {$T \  \text{is} \  NULL $}
      \State return failure
      \EndIf
      
      \If {$S \  \text{is} \  NULL $}
      \State return $Tree \  \text{as a single node with most frequent class label in}\  T$ 
      \EndIf
     
      \If {$\abs{S} = 1 $}
      \State return $Tree \  \text{as a single node}\  S $ 
      \EndIf
      
   \State set $Tree = \{\}$
   
       \For{$a \in S$ }
       \State set $Info(a,T) = 0$  and $SplitInfo(a,T) = 0$
       \State compute $Entropy(a)$
         \For{$v \in values(a,T)$ }
       \State set $T_{a,v}  \text{as the subset of} \  T \  \text{with attribute}\ a = v$  
       \State  $Info(a,T) + = \frac{\abs{T_{a,v}}}{\abs{T_{a}}} Entropy(a)$
       \State $SplitInfo(a,T)+= - \frac{\abs{T_{a,v}}}{\abs{T_{a}}} \log \frac{\abs{T_{a,v}}}{\abs{T_{a}}} $
          \EndFor
          \State $Gain(a,T) = Entropy(a) - Info(a,T)$
          \State  $GainRatio(a,T) = \frac{Gain(a,T)}{SplitInfo(a,T)}$
          \EndFor
       \State set $a_{best} = argmax \{GainRatio(a,T)\}$
       \State $a_{best} \text{into} \  Tree$
        \For{$v \in values(a_{best},T)$ }
       call $C4.5(T_{a,v})$
        \EndFor
        \State return $Tree$
        
  \EndFunction
  

\end{algorithmic}
\end{algorithm}


 

\bibliographystyle{plain}
\bibliography{Bibiliography}


\end{document}



